<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>FDAs</title>
  <link rel="stylesheet" href="style.css">
  <script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true
  },
  options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }
};
</script>
<script id="MathJax-script" async 
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
   <style>
    /* principle 环境样式（外观类似 LaTeX 定理环境） */
    .principle {
      max-width: 900px;
      margin: 1.5em auto;
      padding: 0.9em 1.1em;
      background: #ffffff;
      border-left: 6px solid #2b6cb0; /* 左侧彩色条 */
      box-shadow: 0 2px 8px rgba(20,20,20,0.04);
      border-radius: 6px;
      font-family: "Noto Sans", system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
      color: #111;
      line-height: 1.6;
    }

    .principle-header {
      display: flex;
      align-items: baseline;
      gap: 0.6em;
      margin-bottom: 0.4em;
    }

    .principle-label {
      font-weight: 700;
      color: #0b5394;
      font-size: 0.95em;
    }

    .principle-title {
      font-weight: 600;
      font-size: 1.02em;
      color: #111;
    }

    .principle-body {
      margin-top: 0.35em;
      font-size: 0.98em;
      color: #2c2c2c;
      text-align: justify;
    }

    /* 可选：数学公式居中显示时的样式 */
    .principle .mjx-display {
      text-align: center;
      margin: 0.6em 0;
    }
  </style>
</head>
<body>
  <header>
    <h1>Model Merging with Functioanl Dual Anchor</h1>
    <p class="authors">Kexuan Shi<sup>1</sup>, Yandong Wen<sup>2</sup>, Weiyang Liu<sup>1</sup></p>
    <p class="affiliations">
      <sup>1</sup>The Chinese University of Hong Kong &nbsp;&nbsp;
      <sup>2</sup>Westlake University
    </p>
    <div class="links">
      <a href="#" class="button">[Paper]</a>
      <a href="https://github.com/Sphere-AI-Lab/FDA" class="button" target="_blank">[Code]</a>
      <a href="#" class="button">[BibTeX]</a>
    </div>
  </header>

  <img src="assets/illustration_only.png" alt="FDA Illustration" width="70%" 
     style="display: block; margin: 0 auto;">
  <section id="abstract" style="text-align: center;">
    <h2 style="border-bottom: none; margin-bottom: 0.5em;">
      FDA: A New Framework for Model Merging
    </h2>
    <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
      Model Merging has been an intriguing post-training strategy for integrating knowledge 
      from existing checkpoints of a shared foundation model. Existing methods focus on 
      operations in the parameter space (i.e., task vectors), thereby suffering from the complexity 
      of the parameter space. To explore more knowledge utilizations, we propose 
      <b><i>Functional Dual Anchors (FDAs)</i></b>, a framework (Figure 1(a)) that instead models 
      the knowledge in the input-representation space. Specifically, FDAs are synthetic inputs 
      whose induced gradients align with task vectors, capturing task-specific functional shifts 
      relative to the pretrained model. Then, we use the FDAs to adapt the pretrained model. FDAs provide an alternative perspective on 
      model merging by extending input-space modeling to this setting and bridging joint 
      multi-task training and post-hoc merging.
    </p>
  </section>

  <section id="intuition" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    Intuitive Understanding and Motivation of FDA
  </h2>

      <div style="max-width: 900px; margin: 0 auto; text-align: justify; overflow: hidden;">
      
        <!-- 右侧小图 -->
        <img src="assets/loss_landscape_v3.png" 
             alt="FDA Loss Landscape"
             style="float: right; width: 40%; margin-left: 1em; margin-top: 1em;border-radius: 8px; vertical-align: top;">
      
        <!-- 文字主体（强制与图像顶部对齐） -->
          <p>
            To gain an intuitive understanding of FDAs, we compare their optimization trajectories with those of task arithmetic in Figure 2.
            We treat the obtained FDAs as finetuning data and optimize the model parameters accordingly.
            As shown in the right figure, optimizing with FDAs moves the model closer to the local minima of the loss landscape (computed over eight downstream datasets).
            While task vectors provide useful guidance from the pretrained model, they quickly drift away from the loss basin,
            whereas FDAs consistently guide optimization toward more favorable regions.
            Moreover, by capturing functional shifts in the input space, FDAs offer greater robustness for model merging.
            Unlike task vectors, which are sensitive to initialization and can drift under different starting points,
            FDAs exhibit robustness to such variations, facilitating more reliable model merging.
          </p>
      
      </div>

      <p>
        Another motivation behind FDAs is that modeling the input
        space is generally easier than modeling the parameter space, as the input space tends to be more
        structured. The effectiveness of modeling the input space for knowledge transfer has been extensively
        explored and empirically validated in the context of dataset distillation
        (<sup><a href="#ref-wang2018b">Wang et al., 2018b</a></sup>;
        <sup><a href="#ref-cazenavette2022">Cazenavette et al., 2022</a></sup>),
        iterative teaching (<sup><a href="#ref-liu2017a">Liu et al., 2017a</a></sup>;
        <sup><a href="#ref-qiu2023">Qiu et al., 2023</a></sup>),
        dataset condensation (<sup><a href="#ref-zhao2021">Zhao et al., 2021</a></sup>;
        <sup><a href="#ref-zhao2023">Zhao & Bilen, 2023</a></sup>)
        and continual learning (<sup><a href="#ref-shin2017">Shin et al., 2017</a></sup>;
        <sup><a href="#ref-yu2023">Yu et al., 2023</a></sup>).
      </p>
</section>

  <section id="performance" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    FDAs Provide More Flexible and Robust Merging Direction
  </h2>

  <div style="text-align: center; margin-bottom: 1em;">
    <img src="./docs/assets/fda_performance.png" alt="FDA Performance" width="80%">
    <p style="font-size: small; color: gray;">Figure 2: Performance comparison of FDAs with other model merging methods.</p>
  </div>

  <!-- 正文 -->
  <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
 To validate the effectiveness of the merging direction provided by FDAs, we use FDAs to adapt the pretrained model and compare the multi-task performance 
with its dual framework (i.e., task vectors). To show the robustness, we initialize the pretrained model by the merged parameters, which are derived from data-free
task-vector-based methods. We consider three data-free methods, TA (task arithmetic), TSVM, WUDI. The former one is the classical method, while the latter two are the 
current state-of-the-art methods. We present some results. More results can be found in our paper.
  </p>

  <p style="max-width: 800px; margin: 1em auto 0 auto; text-align: justify;">
    In vision tasks, FDAs achieve comparable or superior accuracy to joint multi-task training while maintaining
    full modularity and no access to original data. For NLP tasks on the GLUE benchmark, FDAs demonstrate smoother
    merging trajectories and less performance degradation under conflicting task updates. These results highlight the robustness and generality of FDAs as a unifying framework for functional model merging.
  </p>
</section>

<section id="algorithm" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    A Practical Algorithm for FDAs
  </h2>

  <!-- 简要介绍 -->
  <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
The practical algorithms for FDAs involve two main stages: constructing FDAs and adapting with FDAs.
In the first stage, FDAs are built for each downstream checkpoint. This stage can be deemed as projecting task-specific knowledge into the input–representation space.
In the second stage, these FDAs are used to adapt the model, i.e., integrate knowledge across multiple tasks.
  </p>

  <!-- 子小节1 -->
  <div style="max-width: 800px; margin: 2em auto 0 auto; text-align: justify;">
    <h3 style="text-align: left; margin-bottom: 0.5em;">1. Construction</h3>
<p>
  Given the pretrained model $\varphi(\boldsymbol{\theta}_0)$ and the corresponding finetuned checkpoint 
  $\varphi(\boldsymbol{\theta}_i)$, we construct the FDAs $\{\boldsymbol{x}_{ij}\}_{j=1}^n$ for $\varphi(\boldsymbol{\theta}_i)$
  via solving the following optimization problem:
</p>
$$
\min_{\mathbf{x}_{i1},\dots,\mathbf{x}_{in}} 
\mathrm{cos\_dist}\!\Bigg(
\nabla_{\boldsymbol{\theta}}\!\sum_{j=1}^n 
\mathrm{Dist}\!\big(\varphi(\boldsymbol{\theta}, \mathbf{x}_{ij}), 
\varphi(\boldsymbol{\theta}_i, \mathbf{x}_{ij})\big)
\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_0}, 
\boldsymbol{\tau}_i
\Bigg)
$$

<p>
where $\mathrm{cos\_dist}(\mathbf{A},\mathbf{B}) = 1 - 
\frac{\mathrm{vec}(\mathbf{A})^\top \mathrm{vec}(\mathbf{B})}
{\|\mathbf{A}\|_F \|\mathbf{B}\|_F}$, 
$\mathrm{vec}$ denotes the operation that vectorizes a matrix into a vector 
in row-major order, and $\mathrm{Dist}(\cdot)$ denotes a differentiable distance 
function measuring the representation discrepancy between 
$\varphi(\boldsymbol{\theta}_0)$ and $\varphi(\boldsymbol{\theta}_i)$. 
</p>
<p>
  The gradient-based iterative optimization methods are adopted. It is well known that gradient-based methods are sensitive to initialization. Thus, we analyze the optimization dynamics of anchors on a linear encoder and derive a principle for
  initialization. 
  <div class="Initialization Principle" role="region" aria-labelledby="principle-1-title">
    <div class="principle-header">
      <div class="principle-label">Principle</div>
      <div id="principle-1-title" class="principle-title"></div>
    </div>

    <div class="principle-body">
      <p>
        An effective initialization strategy should limit the energy of the initialization point within the tail subspace spanned by the task vector.
      </p>
    </div>
  </div>
</p>
    <p>
      Based on this principle, we derive two practical initialization schemes: linear weight sampling $\boldsymbol{x}_{ij}=(\boldsymbol{W}_i)_{l_j,:}\$ and scaled Gaussian sampling $\vx_{ij} = \sigma \cdot \tilde{\boldsymbol{x}}_{ij},
\tilde{\boldsymbol{x}}_{ij} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}_d)$, where $\boldsymbol{W}$ denotes the weight matrix. 
    </p>
  </div>

  <!-- 子小节2 -->
  <div style="max-width: 800px; margin: 2em auto 0 auto; text-align: justify;">
    <h3 style="text-align: left; margin-bottom: 0.5em;">2. Adaptation</h3>
    <p>
      The adaptation process with FDAs is the dual process of the above Construction process. When the merged model is initialized by the pretrained checkpoint,
      the adaptation process is to optimize the following objective:
      $$
\min_{\boldsymbol{\theta_0}} 
\sum_{i=1}^m \sum_{j=1}^{n} 
\mathrm{Dist}\!\Big( 
\varphi(\boldsymbol{\theta_0}, \mathbf{x}_{ij}),
\varphi(\boldsymbol{\theta}_i, \mathbf{x}_{ij})
\Big),
$$
      
    </p>
  </div>
</section>


    <section id="performance" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    Knowledge encoded in the FDAs
  </h2>

  <!-- 图片展示区（可换成你的性能图表） -->
  <div style="text-align: center; margin-bottom: 1em;">
    <img src="./docs/assets/fda_performance.png" alt="FDA Performance" width="80%">
    <p style="font-size: small; color: gray;">Figure 2: Performance comparison of FDAs with other model merging methods.</p>
  </div>

  <!-- 正文 -->
  <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
    To evaluate the effectiveness of <b><i>Functional Dual Anchors (FDAs)</i></b>, we conduct extensive experiments across
    multiple modalities, including vision, natural language understanding (NLP), and natural language generation (NLG).
    FDAs consistently outperform traditional parameter-space merging methods such as Task Arithmetic, DARE, and WUDI
    across a wide range of datasets and architectures.
  </p>

  <p style="max-width: 800px; margin: 1em auto 0 auto; text-align: justify;">
    In vision tasks, FDAs achieve comparable or superior accuracy to joint multi-task training while maintaining
    full modularity and no access to original data. For NLP tasks on the GLUE benchmark, FDAs demonstrate smoother
    merging trajectories and less performance degradation under conflicting task updates. For large-scale language
    models, FDAs enable effective functional adaptation between experts in domains such as mathematics and code.
    These results highlight the robustness and generality of FDAs as a unifying framework for functional model merging.
  </p>
</section>

  <section id="citation">
    <h2>BibTeX</h2>
    <pre>
@inproceedings{shi2026fda,
  title={MODEL MERGING WITH FUNCTIONAL DUAL ANCHORS},
  author={Shi, Kexuan and Wen, Yandong and Liu, Weiyang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026}
}
    </pre>
  </section>

    <section id="references" style="text-align:left; max-width:800px; margin: 2em auto;">
  <h3>References</h3>
  <ol>
    <li id="ref-wang2018b">Wang, T., Zhu, J. et al. (2018b). Dataset Distillation. <i>ICLR</i>.</li>
    <li id="ref-cazenavette2022">Cazenavette, G., et al. (2022). Dataset Condensation with Gradient Matching. <i>CVPR</i>.</li>
    <li id="ref-liu2017a">Liu, Y., et al. (2017a). Iterative Machine Teaching. <i>NeurIPS</i>.</li>
    <li id="ref-qiu2023">Qiu, X., et al. (2023). Teaching via Iterative Distillation. <i>ICML</i>.</li>
    <li id="ref-zhao2021">Zhao, B., et al. (2021). Dataset Condensation with Gradient Matching. <i>ICLR</i>.</li>
    <li id="ref-zhao2023">Zhao, B., & Bilen, H. (2023). Synthesizing Informative Data for Efficient Learning. <i>CVPR</i>.</li>
    <li id="ref-shin2017">Shin, H., et al. (2017). Continual Learning with Deep Generative Replay. <i>NeurIPS</i>.</li>
    <li id="ref-yu2023">Yu, C., et al. (2023). Continual Learning with Functional Regularization. <i>ICML</i>.</li>
  </ol>
</section>

  <footer>
    <p>This website is adapted from Nerfies, MathVista and SGP-Bench, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p>
  </footer>
</body>
</html>
