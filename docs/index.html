<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>FDA</title>
  <link rel="stylesheet" href="style.css">
  <script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true
  },
  options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }
};
</script>
<script id="MathJax-script" async 
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
   <style>
    /* principle 环境样式（外观类似 LaTeX 定理环境） */
    .principle {
      max-width: 900px;
      margin: 1.5em auto;
      padding: 0.9em 1.1em;
      background: #ffffff;
      border-left: 6px solid #2b6cb0; /* 左侧彩色条 */
      box-shadow: 0 2px 8px rgba(20,20,20,0.04);
      border-radius: 6px;
      font-family: "Noto Sans", system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
      color: #111;
      line-height: 1.6;
    }

    .principle-header {
      display: flex;
      align-items: baseline;
      gap: 0.6em;
      margin-bottom: 0.4em;
    }

    .principle-label {
      font-weight: 700;
      color: #0b5394;
      font-size: 0.95em;
    }

    .principle-title {
      font-weight: 600;
      font-size: 1.02em;
      color: #111;
    }

    .principle-body {
      margin-top: 0.35em;
      font-size: 0.98em;
      color: #2c2c2c;
      text-align: justify;
    }

    /* 可选：数学公式居中显示时的样式 */
    .principle .mjx-display {
      text-align: center;
      margin: 0.6em 0;
    }
  </style>
</head>
<body>
  <header>
    <h1>Model Merging with Functioanl Dual Anchor</h1>
    <p class="authors">Kexuan Shi<sup>1</sup>, Yandong Wen<sup>2</sup>, Weiyang Liu<sup>1</sup></p>
    <p class="affiliations">
      <sup>1</sup>The Chinese University of Hong Kong &nbsp;&nbsp;
      <sup>2</sup>Westlake University
    </p>
    <div class="links">
      <a href="#" class="button">Paper</a>
      <a href="https://github.com/Sphere-AI-Lab/FDA" class="button" target="_blank">Code</a>
    </div>
  </header>

  <img src="assets/illustration_only.png" alt="FDA Illustration" width="70%" 
     style="display: block; margin: 0 auto;">
  <section id="abstract" style="text-align: center;">
    <h2 style="border-bottom: none; margin-bottom: 0.5em;">
      FDA: A New Framework for Model Merging
    </h2>
    <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
      Model Merging has been an intriguing post-training strategy for integrating knowledge 
      from existing checkpoints of a shared foundation model. Existing methods focus on 
      operations in the parameter space (i.e., task vectors), thereby suffering from the complexity 
      of the parameter space. To explore more knowledge utilizations, we propose 
      <b><i>Functional Dual Anchors (FDAs)</i></b>, a framework (Figure 1(a)) that instead models 
      the knowledge in the input-representation space. Specifically, FDAs are synthetic inputs 
      whose induced gradients align with task vectors, capturing task-specific functional shifts 
      relative to the pretrained model. Then, we use the FDAs to adapt the pretrained model. FDAs provide an alternative perspective on 
      model merging by extending input-space modeling to this setting and bridging joint 
      multi-task training and post-hoc merging.
    </p>
  </section>

  <section id="intuition" style="text-align: center; margin-top: 3em;">
<h2 style="
  border-bottom: none;
  margin-bottom: 0.5em;
  background-color: #f5f5f5;
  padding: 0.8em 0;
  width: 100%;
">
  Intuitive Understanding and Motivation of FDA
</h2>

      <div style="max-width: 900px; margin: 0 auto; text-align: justify; overflow: hidden;">
      
        <img src="assets/loss_landscape_v3.png" 
             alt="FDA Loss Landscape"
             style="float: right; width: 40%; margin-left: 1em; margin-top: 1em;border-radius: 8px; vertical-align: top;">
      
          <p>
            To gain an intuitive understanding of FDAs, we compare their optimization trajectories with those of task arithmetic in Figure 2.
            We treat the obtained FDAs as finetuning data and optimize the model parameters accordingly.
            As shown in the right figure, optimizing with FDAs moves the model closer to the local minima of the loss landscape (computed over eight downstream datasets).
            While task vectors provide useful guidance from the pretrained model, they quickly drift away from the loss basin,
            whereas FDAs consistently guide optimization toward more favorable regions.
            Moreover, by capturing functional shifts in the input space, FDAs offer greater robustness for model merging.
            Unlike task vectors, which are sensitive to initialization and can drift under different starting points,
            FDAs exhibit robustness to such variations, facilitating more reliable model merging.
          </p>

         <p>
        Another motivation behind FDAs is that modeling the input
        space is generally easier than modeling the parameter space, as the input space tends to be more
        structured. The effectiveness of modeling the input space for knowledge transfer has been extensively
        explored and empirically validated in the context of dataset distillation
        (<sup><a href="#ref-wang2018b">Wang et al., 2018b</a></sup>;
        <sup><a href="#ref-cazenavette2022">Cazenavette et al., 2022</a></sup>),
        iterative teaching (<sup><a href="#ref-liu2017a">Liu et al., 2017a</a></sup>;
        <sup><a href="#ref-qiu2023">Qiu et al., 2023</a></sup>),
        dataset condensation (<sup><a href="#ref-zhao2021">Zhao et al., 2021</a></sup>;
        <sup><a href="#ref-zhao2023">Zhao & Bilen, 2023</a></sup>)
        and continual learning (<sup><a href="#ref-shin2017">Shin et al., 2017</a></sup>;
        <sup><a href="#ref-yu2023">Yu et al., 2023</a></sup>).
      </p>
      
      </div>
</section>

  <section id="performance" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    FDAs Provide More Flexible and Robust Merging Direction
  </h2>
<table style="width:100%; border-collapse: collapse; font-size: 12px; text-align: center; margin-top: 1em;">
  <thead style="background-color: #f5f5f5;">
    <tr>
      <th style="border: 1px solid #ddd; padding: 6px;">Method</th>
      <th>CoLA</th><th>SST-2</th><th>MRPC</th><th>STS-B</th><th>QQP</th>
      <th>MNLI</th><th>QNLI</th><th>RTE</th><th>Avg</th><th>&Delta;</th>
    </tr>
  </thead>

  <tbody>
    <tr><td style="text-align:left;">Pretrained</td><td>0.1679</td><td>0.4897</td><td>0.7480</td><td>-0.0471</td><td>0.3159</td><td>0.3545</td><td>0.5054</td><td>0.4693</td><td>0.3754</td><td>-</td></tr>
    <tr><td style="text-align:left;">Individual</td><td>0.6335</td><td>0.9001</td><td>0.9224</td><td>0.9418</td><td>0.9055</td><td>0.8267</td><td>0.9507</td><td>0.9222</td><td>0.8754</td><td>-</td></tr>

    <tr><td colspan="11" style="border-bottom:1px solid #bbb;"></td></tr>

    <tr><td style="text-align:left;">RegMean</td><td>0.3449</td><td>0.8922</td><td>0.5949</td><td>0.3509</td><td>0.8045</td><td>0.5894</td><td>0.6132</td><td>0.6534</td><td>0.6054</td><td>-</td></tr>
    <tr><td style="text-align:left;">Fisher merging</td><td>0.2700</td><td>0.7856</td><td>0.7517</td><td>0.2624</td><td>0.3159</td><td>0.4385</td><td>0.5367</td><td>0.6426</td><td>0.5004</td><td>-</td></tr>
    <tr><td style="text-align:left;">AdaMerging</td><td>0.1027</td><td>0.9335</td><td>0.7480</td><td><b>0.7432</b></td><td>0.3159</td><td>0.7506</td><td>0.8578</td><td>0.6245</td><td>0.6345</td><td>-</td></tr>
    <tr><td style="text-align:left;">ProDistill</td><td><b>0.4833</b></td><td><b>0.9427</b></td><td><b>0.8655</b></td><td>0.7310</td><td><b>0.8269</b></td><td><b>0.8122</b></td><td><b>0.8825</b></td><td><b>0.7545</b></td><td><b>0.7873</b></td><td>-</td></tr>

    <tr><td colspan="11" style="border-bottom:1px solid #bbb;"></td></tr>

    <tr><td style="text-align:left;">TA</td><td>0.1635</td><td>0.8716</td><td>0.7480</td><td>0.6603</td><td>0.3159</td><td>0.6101</td><td>0.8716</td><td>0.7366</td><td>0.5918</td><td>-</td></tr>
    <tr><td style="text-align:left;">TSV</td><td>0.4791</td><td>0.9323</td><td>0.7459</td><td>0.6660</td><td>0.3300</td><td>0.6750</td><td>0.7761</td><td>0.6751</td><td>0.6599</td><td>-</td></tr>
    <tr><td style="text-align:left;">WUDI</td><td>0.4201</td><td>0.9232</td><td>0.7487</td><td>0.7345</td><td>0.5393</td><td>0.6430</td><td>0.5746</td><td>0.5740</td><td>0.6447</td><td>-</td></tr>

    <!-- 灰色行：FDAs 系列 -->
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (Pretrained, Gauss)</td><td>0.3198</td><td>0.8463</td><td>0.7790</td><td>0.6828</td><td><b>0.7423</b></td><td>0.5605</td><td>0.6021</td><td><b>0.7726</b></td><td>0.6632</td><td style="color:#006400;">+0.2878</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (Pretrained, Weight)</td><td>0.3883</td><td>0.8911</td><td><b>0.7858</b></td><td>0.7230</td><td>0.7410</td><td>0.5791</td><td>0.6207</td><td>0.7329</td><td>0.6827</td><td style="color:#006400;">+0.3073</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (TA, Gauss)</td><td>0.4043</td><td><b>0.9461</b></td><td>0.7692</td><td>0.7897</td><td>0.6916</td><td>0.7190</td><td>0.7487</td><td>0.7076</td><td><b>0.7220</b></td><td style="color:#006400;">+0.1302</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (TA, Weight)</td><td>0.4511</td><td>0.9404</td><td>0.7578</td><td>0.7926</td><td>0.6518</td><td><b>0.7411</b></td><td>0.6965</td><td>0.7148</td><td>0.7183</td><td style="color:#006400;">+0.1265</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (TSV, Gauss)</td><td><b>0.5036</b></td><td>0.9438</td><td>0.7521</td><td><b>0.7975</b></td><td>0.4128</td><td>0.7075</td><td><b>0.8477</b></td><td>0.7365</td><td>0.7127</td><td style="color:#006400;">+0.0528</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (TSV, Weight)</td><td>0.5021</td><td>0.9427</td><td>0.7490</td><td>0.7418</td><td>0.5062</td><td>0.7292</td><td>0.8146</td><td>0.7365</td><td>0.7153</td><td style="color:#006400;">+0.0554</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (WUDI, Gauss)</td><td>0.4841</td><td>0.9404</td><td>0.7647</td><td>0.7645</td><td>0.6778</td><td>0.7004</td><td>0.5911</td><td>0.6643</td><td>0.6984</td><td style="color:#006400;">+0.0537</td></tr>
    <tr style="background-color:#f0f0f0;"><td style="text-align:left;">FDAs (WUDI, Weight)</td><td>0.4848</td><td>0.9392</td><td>0.7573</td><td>0.7546</td><td>0.6979</td><td>0.7072</td><td>0.5656</td><td>0.6643</td><td>0.6964</td><td style="color:#006400;">+0.0517</td></tr>
  </tbody>
</table>

<p style="font-size: 11px; color: #555; text-align: center; max-width: 900px; margin: 0 auto; margin-top: 0.5em;">
  Performance of merging RoBERTa-Large models across eight NLU tasks. The second section (from RegMean to ProDistill) includes methods that use task-specific data, 
  and the third section is data-free methods. “FDA (<i>init model</i>, <i>FDA init</i>)” denotes the choice of the initial model and the initialization strategies 
  for FDAs, respectively. “Δ” denotes the performance improvement compared to the initial model.
</p>

  <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
 To validate the effectiveness of the merging direction provided by FDAs, we use FDAs to adapt the pretrained model and compare the multi-task performance 
with its dual framework (i.e., task vectors). To show the robustness, we initialize the pretrained model by the merged parameters, which are derived from data-free
task-vector-based methods. We consider three data-free methods, TA (task arithmetic), TSVM, WUDI. The former one is the classical method, while the latter two are the 
current state-of-the-art methods. We present some results. More results can be found in our paper.
  </p>

  <p style="max-width: 800px; margin: 1em auto 0 auto; text-align: justify;">
    In vision tasks, FDAs achieve comparable or superior accuracy to joint multi-task training while maintaining
    full modularity and no access to original data. For NLP tasks on the GLUE benchmark, FDAs demonstrate smoother
    merging trajectories and less performance degradation under conflicting task updates. These results highlight the robustness and generality of FDAs as a unifying framework for functional model merging.
  </p>
</section>

<section id="algorithm" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    A Practical Algorithm for FDAs
  </h2>

  <!-- 简要介绍 -->
  <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
The practical algorithms for FDAs involve two main stages: constructing FDAs and adapting with FDAs.
In the first stage, FDAs are built for each downstream checkpoint. This stage can be deemed as projecting task-specific knowledge into the input–representation space.
In the second stage, these FDAs are used to adapt the model, i.e., integrate knowledge across multiple tasks.
  </p>

  <!-- 子小节1 -->
  <div style="max-width: 800px; margin: 2em auto 0 auto; text-align: justify;">
    <h3 style="text-align: left; margin-bottom: 0.5em;">1. Construction</h3>
<p>
  Given the pretrained model $\varphi(\boldsymbol{\theta}_0)$ and the corresponding finetuned checkpoint 
  $\varphi(\boldsymbol{\theta}_i)$, we construct the FDAs $\{\boldsymbol{x}_{ij}\}_{j=1}^n$ for $\varphi(\boldsymbol{\theta}_i)$
  via solving the following optimization problem:
</p>
$$
\min_{\mathbf{x}_{i1},\dots,\mathbf{x}_{in}} 
\mathrm{cos\_dist}\!\Bigg(
\nabla_{\boldsymbol{\theta}}\!\sum_{j=1}^n 
\mathrm{Dist}\!\big(\varphi(\boldsymbol{\theta}, \mathbf{x}_{ij}), 
\varphi(\boldsymbol{\theta}_i, \mathbf{x}_{ij})\big)
\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_0}, 
\boldsymbol{\tau}_i
\Bigg)
$$

<p>
where $\mathrm{cos\_dist}(\mathbf{A},\mathbf{B}) = 1 - 
\frac{\mathrm{vec}(\mathbf{A})^\top \mathrm{vec}(\mathbf{B})}
{\|\mathbf{A}\|_F \|\mathbf{B}\|_F}$, 
$\mathrm{vec}$ denotes the operation that vectorizes a matrix into a vector 
in row-major order, and $\mathrm{Dist}(\cdot)$ denotes a differentiable distance 
function measuring the representation discrepancy between 
$\varphi(\boldsymbol{\theta}_0)$ and $\varphi(\boldsymbol{\theta}_i)$. 
</p>
<p>
  The gradient-based iterative optimization methods are adopted. It is well known that gradient-based methods are sensitive to initialization. Thus, we analyze the optimization dynamics of anchors on a linear encoder and derive a principle for
  initialization. 
  <div class="principle" role="region" aria-labelledby="principle-1-title">
    <div class="principle-header">
      <div class="principle-label">Principle</div>
      <div id="principle-1-title" class="principle-title"></div>
    </div>

    <div class="principle-body">
      <p>
        An effective initialization strategy should limit the energy of the initialization point within the tail subspace spanned by the task vector.
      </p>
    </div>
  </div>
</p>
    <p>
      Based on this principle, we derive two practical initialization schemes: linear weight sampling $\boldsymbol{x}_{ij}=(\boldsymbol{W}_i)_{l_j,:}$ and scaled Gaussian sampling $\boldsymbol{x}_{ij} = \sigma \cdot \tilde{\boldsymbol{x}}_{ij},
\tilde{\boldsymbol{x}}_{ij} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I}_d)$, where $\boldsymbol{W}$ denotes the weight matrix. 
    </p>
  </div>

  <div style="max-width: 800px; margin: 2em auto 0 auto; text-align: justify;">
    <h3 style="text-align: left; margin-bottom: 0.5em;">2. Adaptation</h3>
    <p>
      The adaptation process with FDAs is the dual process of the above Construction process. When the merged model is initialized by the pretrained checkpoint,
      the adaptation process is to optimize the following objective:
      $$
\min_{\boldsymbol{\theta_0}} 
\sum_{i=1}^m \sum_{j=1}^{n} 
\mathrm{Dist}\!\Big( 
\varphi(\boldsymbol{\theta_0}, \mathbf{x}_{ij}),
\varphi(\boldsymbol{\theta}_i, \mathbf{x}_{ij})
\Big).
$$
When the merged model is initialized by the merged parameters from task-vector-based methods, the adaptation process is to refine the merged task vectors. The objective
      is as follows:
      $$
\min_{\{\phi(\boldsymbol{\tau}_i)\}_{i=1}^m} 
\sum_{i=1}^m \sum_{j=1}^{n} 
\mathrm{Dist}\!\Big( 
\varphi \big(\boldsymbol{\theta} + \sum_{i=1}^m \phi_i(\boldsymbol{\tau}_i), \mathbf{x}_{ij}\big),
\varphi(\boldsymbol{\theta}_i, \mathbf{x}_{ij})
\Big).
$$ 
  </p>
  </div>
</section>


    <section id="knowledge" style="text-align: center; margin-top: 3em;">
  <h2 style="border-bottom: none; margin-bottom: 0.5em;">
    Knowledge encoded in the FDAs
  </h2>
  <p style="max-width: 800px; margin: 0 auto; text-align: justify;">
Since FDAs projects the task-specific knowledge into the input-representation space, we investigate the knowledge encoded by FDAs. We made three 
interesting observations.
  </p>
<div style="max-width: 800px; margin: 1.5em auto; text-align: justify;">
  <p><b>Observation 1 – FDAs evolve into a long-tailed spectrum structure during optimization:</b> We perform SVD on the FDA matrices and normalize singular values by the largest one. From the Figure, the normalized tail
   singular values decays rapidly in construction process for different initializations. This phenomenon is reasonable, as task-specific knowledge absorption often manifests as a long-tailed, low-rank structure in the parameter space as well. </p>
  <p><b>Observation 2 – The high-energy subspaces of FDAs gradually aligns with that of real data:</b> Considering the long-tailed structure of FDAs, we measure subspace similarity of top $20\%$ singular vectores between real data and FDAs via Projection Matrix. From the examples in Figure, the similarity
  gradually increases as the optimization proceeds. This suggests a potential connection between the knowledge encoded in FDAs and the real task data. </p>
  <p><b>Observation 3 – FDAs-induced adaptation increasingly aligns with that induced by real data:</b> We analyze FDAs here by re-projecting them into the parameter space, i.e., the adaptation they induce. We project the FDA-induced adaptation onto a non-negative cone spanned by parameter updation vectors derived from real data. As shown in Figure,
  the projection energy gradually increases in both pretrained model and merged model. This indicates that FDAs progressively produce the robust task-specific functional shifts.
  </p>
</div>

</section>

  <section id="citation">
    <h2>BibTeX</h2>
    <pre>
@inproceedings{shi2026fda,
  title={MODEL MERGING WITH FUNCTIONAL DUAL ANCHORS},
  author={Shi, Kexuan and Wen, Yandong and Liu, Weiyang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026}
}
    </pre>
  </section>

    <section id="references" style="text-align:left; max-width:800px; margin: 2em auto;">
  <h3>References</h3>
  <ol>
    <li id="ref-wang2018b">Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. </li>
    <li id="ref-cazenavette2022">George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset
distillation by matching training trajectories. </li>
    <li id="ref-liu2017a">Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg,
and Le Song. Iterative machine teaching. </li>
    <li id="ref-qiu2023"> Zeju Qiu, Weiyang Liu, Tim Z Xiao, Zhen Liu, Umang Bhatt, Yucen Luo, Adrian Weller, and
Bernhard Sch¨olkopf. Iterative teaching by data hallucination. </li>
    <li id="ref-zhao2021">Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. </li>
    <li id="ref-zhao2023">Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching.</li>
    <li id="ref-shin2017">Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. </li>
    <li id="ref-yu2023">Longhui Yu, Tianyang Hu, Lanqing Hong, Zhen Liu, Adrian Weller, and Weiyang Liu. Continual
learning by modeling intra-class variation.</li>
  </ol>
</section>

  <footer>
    <p>This website is adapted from Nerfies, MathVista and SGP-Bench, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p>
  </footer>
</body>
</html>
